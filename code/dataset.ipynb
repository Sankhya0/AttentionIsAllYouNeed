{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stucture of Dataset\n",
    "\n",
    "Note that we are working with the Huggingface en-it opus books dataset.\n",
    "\n",
    "1. In this item has an \"id\" and a \"translation\"\n",
    "2. The id is just an int (index)\n",
    "3. The translation is a dictionary in itself\n",
    "4. In our case, the src_lang = \"en\" and tgt_lang = \"it\" which will be the keys of the \"translation\" dictionary\n",
    "5. Therefore, we can use `[translation][src_lang]` to see the text in the ds item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilingualDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        self.sos_token = torch.Tensor([tokenizer_src.token_to_id(['[SOS]'])], dtype = torch.int64)\n",
    "        self.eos_token = torch.Tensor([tokenizer_src.token_to_id(['[EOS]'])], dtype = torch.int64)\n",
    "        self.pad_token = torch.Tensor([tokenizer_src.token_to_id(['[PAD]'])], dtype = torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, index: Any) -> Any:\n",
    "        \n",
    "        # Get the ds item\n",
    "        src_target_pair = self.ds[index]\n",
    "\n",
    "        # Get the source and target text from the item\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "\n",
    "        # src words --> tokens --> id of the tokens\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # getting the number of padding tokens required\n",
    "        # note that we are doing -2 to the encoder tokens because the sos and eos have to be accounted for\n",
    "        # but in the decoder the EOS doesnt have to be there as it would be predicted by the model so we only do a -1\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "\n",
    "        # if the padding is less than 0, then there is a problem\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "        \n",
    "        # Add SOS, EOS and padding tokens to the source text\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype = torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype = torch.int64)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Add SOS token to the decoder input\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype = torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Add EOD token to the label (Expected in the output)\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype = torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        assert encoder_input.shape[0] == self.seq_len\n",
    "        assert decoder_input.shape[0] == self.seq_len\n",
    "        assert label.shape[0] == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input, # (seq_len)\n",
    "            \"decoder_input\": decoder_input, # (seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # [1, 1, seq_len] to account for the batch dimension\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # [1,1,seq_len] & [1,seq_len]\n",
    "            \"label\": label,\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text\n",
    "\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(size):\n",
    "    # Keeps only the upper triangular part, setting elements above the main diagonal to 1, while the rest remains 0.\n",
    "    mask = torch.triu(torch.ones(1,size,size), diagonal = 1).int()\n",
    "\n",
    "    # we want the opposite, i.e. above the diagonal to be 0\n",
    "    return mask == 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
